{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COMP579 Assignment 2\n",
    "\n",
    "Authors:\n",
    "* Ryan Reszetnik: 260948454\n",
    "* Mathieu Geoffroy: 260986559\n",
    "\n",
    "**Coding: Tabular RL [70 points]**"
   ],
   "metadata": {
    "id": "ALnFQQHn1QME"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "VlHR8XcY483f",
    "ExecuteTime": {
     "end_time": "2024-02-19T05:44:39.798733Z",
     "start_time": "2024-02-19T05:44:39.664038Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def softmax(x, temp):\n",
    "    # write your solution here\n",
    "    e = np.exp(x / temp)\n",
    "    return e / e.sum()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4KZyIS-5LxK",
    "outputId": "e074b368-abb2-49f2-c3fd-851279cdb8ed",
    "ExecuteTime": {
     "end_time": "2024-02-19T05:44:39.910877Z",
     "start_time": "2024-02-19T05:44:39.795394Z"
    }
   },
   "execution_count": 88,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        # write your solution here\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.temp = temp\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def select_action(self, s, greedy=False):\n",
    "        # write your solution here\n",
    "        if greedy:\n",
    "            return np.argmax(self.Q[s, :])\n",
    "        else:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "    def update(self, s, a, r, s_prime, a_prime, done):\n",
    "        # write your solution here\n",
    "        prediction = self.Q[s, a]\n",
    "        target = r + self.gamma * self.Q[s_prime, a_prime] * (1 - done)\n",
    "        self.Q[s, a] += self.alpha * (target - prediction)\n",
    "        return self.Q\n",
    "\n",
    "\n",
    "class ExpectedSarsa:\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        # write your solution here\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.temp = temp\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def select_action(self, s, greedy=False):\n",
    "        # write your solution here\n",
    "        if greedy:\n",
    "            # if finished training, then choose the optimal policy\n",
    "            return np.argmax(self.Q[s, :])\n",
    "        else:\n",
    "            return np.random.choice(self.env.action_space.n, p=softmax(self.Q[s, :], self.temp))\n",
    "\n",
    "    def update(self, s, a, r, s_prime, a_prime, done):\n",
    "        prediction = self.Q[s, a]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# bonus question, optional\n",
    "class Hybrid_Sarsa_Q:\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        # write your solution here\n",
    "        self.env = None\n",
    "        self.alpha = None\n",
    "        self.gamma = None\n",
    "        self.temp = None\n",
    "        self.Q = None\n",
    "        return\n",
    "\n",
    "    def select_action(self, s, greedy=False):\n",
    "        # write your solution here\n",
    "        if greedy:\n",
    "            # if finished training, then choose the optimal policy\n",
    "            return\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def update(self, s, a, r, s_prime, a_prime, done):\n",
    "        # write your solution here\n",
    "        return"
   ],
   "metadata": {
    "id": "gttYBrHj5Yal",
    "ExecuteTime": {
     "end_time": "2024-02-19T05:44:40.031769Z",
     "start_time": "2024-02-19T05:44:39.917022Z"
    }
   },
   "execution_count": 89,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write your experiment code below"
   ],
   "metadata": {
    "id": "Sazk2zxT8jD7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"State space:\", env.observation_space)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OevfjgjL8mFk",
    "outputId": "49142b56-db73-4558-c0a9-96acde134295",
    "ExecuteTime": {
     "end_time": "2024-02-19T05:44:40.157254Z",
     "start_time": "2024-02-19T05:44:40.031589Z"
    }
   },
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "State space: Discrete(500)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# function that runs each episode\n",
    "def run_episode(agent, env, max_steps, train=False):\n",
    "    s = env.reset()[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    a = agent.select_action(s, not train)\n",
    "    for step in range(max_steps):\n",
    "        s_prime, r, done, info, mask = env.step(a)\n",
    "        a_prime = agent.select_action(s_prime, not train)\n",
    "        if train:\n",
    "            agent.update(s, a, r, s_prime, a_prime, done)\n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "        episode_reward += r\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return episode_reward\n",
    "            \n",
    "# function that runs each hyperparameter setting\n",
    "def run_experiment(agent, env, num_segments, max_steps):\n",
    "    rewards = np.zeros(num_segments)\n",
    "    for i in range(num_segments):\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            rewards[i] = run_episode(agent, env, max_steps, train=False)\n",
    "        else:\n",
    "            rewards[i] = run_episode(agent, env, max_steps, train=True)\n",
    "    return rewards\n"
   ],
   "metadata": {
    "id": "NWTmONa1BgaV",
    "ExecuteTime": {
     "end_time": "2024-02-19T05:44:40.274838Z",
     "start_time": "2024-02-19T05:44:40.151017Z"
    }
   },
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for sarsa with alpha=0.1, gamma=0.1, temp=0.1...\n",
      "\tTrial 0 Reward: -48.0\n",
      "\tTrial 1 Reward: -157.0\n",
      "\tTrial 2 Reward: -33.0\n",
      "\tTrial 3 Reward: -107.0\n",
      "\tTrial 4 Reward: -64.0\n",
      "\tTrial 5 Reward: -159.0\n",
      "\tTrial 6 Reward: -104.0\n",
      "\tTrial 7 Reward: -90.0\n",
      "\tTrial 8 Reward: -57.0\n",
      "\tTrial 9 Reward: -96.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.1, temp=0.5...\n",
      "\tTrial 0 Reward: -98.0\n",
      "\tTrial 1 Reward: -102.0\n",
      "\tTrial 2 Reward: -29.0\n",
      "\tTrial 3 Reward: -154.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -169.0\n",
      "\tTrial 6 Reward: -75.0\n",
      "\tTrial 7 Reward: -73.0\n",
      "\tTrial 8 Reward: -73.0\n",
      "\tTrial 9 Reward: -192.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.1, temp=0.9...\n",
      "\tTrial 0 Reward: -153.0\n",
      "\tTrial 1 Reward: -60.0\n",
      "\tTrial 2 Reward: -186.0\n",
      "\tTrial 3 Reward: -84.0\n",
      "\tTrial 4 Reward: -151.0\n",
      "\tTrial 5 Reward: -20.0\n",
      "\tTrial 6 Reward: -98.0\n",
      "\tTrial 7 Reward: -40.0\n",
      "\tTrial 8 Reward: -99.0\n",
      "\tTrial 9 Reward: -119.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.1, temp=0.99...\n",
      "\tTrial 0 Reward: -71.0\n",
      "\tTrial 1 Reward: -69.0\n",
      "\tTrial 2 Reward: -155.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -89.0\n",
      "\tTrial 5 Reward: -65.0\n",
      "\tTrial 6 Reward: -131.0\n",
      "\tTrial 7 Reward: -87.0\n",
      "\tTrial 8 Reward: -34.0\n",
      "\tTrial 9 Reward: -200.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.5, temp=0.1...\n",
      "\tTrial 0 Reward: -63.0\n",
      "\tTrial 1 Reward: -48.0\n",
      "\tTrial 2 Reward: -172.0\n",
      "\tTrial 3 Reward: -55.0\n",
      "\tTrial 4 Reward: -181.0\n",
      "\tTrial 5 Reward: -58.0\n",
      "\tTrial 6 Reward: -5.0\n",
      "\tTrial 7 Reward: -79.0\n",
      "\tTrial 8 Reward: -165.0\n",
      "\tTrial 9 Reward: -77.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.5, temp=0.5...\n",
      "\tTrial 0 Reward: -51.0\n",
      "\tTrial 1 Reward: -94.0\n",
      "\tTrial 2 Reward: -17.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -79.0\n",
      "\tTrial 5 Reward: -100.0\n",
      "\tTrial 6 Reward: -149.0\n",
      "\tTrial 7 Reward: -157.0\n",
      "\tTrial 8 Reward: -117.0\n",
      "\tTrial 9 Reward: -62.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.5, temp=0.9...\n",
      "\tTrial 0 Reward: -101.0\n",
      "\tTrial 1 Reward: -45.0\n",
      "\tTrial 2 Reward: -28.0\n",
      "\tTrial 3 Reward: -50.0\n",
      "\tTrial 4 Reward: -80.0\n",
      "\tTrial 5 Reward: -30.0\n",
      "\tTrial 6 Reward: -200.0\n",
      "\tTrial 7 Reward: -108.0\n",
      "\tTrial 8 Reward: -42.0\n",
      "\tTrial 9 Reward: -55.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.5, temp=0.99...\n",
      "\tTrial 0 Reward: -183.0\n",
      "\tTrial 1 Reward: -86.0\n",
      "\tTrial 2 Reward: -14.0\n",
      "\tTrial 3 Reward: -169.0\n",
      "\tTrial 4 Reward: -118.0\n",
      "\tTrial 5 Reward: -151.0\n",
      "\tTrial 6 Reward: -114.0\n",
      "\tTrial 7 Reward: -95.0\n",
      "\tTrial 8 Reward: -27.0\n",
      "\tTrial 9 Reward: -200.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.9, temp=0.1...\n",
      "\tTrial 0 Reward: -111.0\n",
      "\tTrial 1 Reward: -159.0\n",
      "\tTrial 2 Reward: -102.0\n",
      "\tTrial 3 Reward: 10.0\n",
      "\tTrial 4 Reward: 14.0\n",
      "\tTrial 5 Reward: 15.0\n",
      "\tTrial 6 Reward: -136.0\n",
      "\tTrial 7 Reward: 12.0\n",
      "\tTrial 8 Reward: -88.0\n",
      "\tTrial 9 Reward: 6.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.9, temp=0.5...\n",
      "\tTrial 0 Reward: -174.0\n",
      "\tTrial 1 Reward: -185.0\n",
      "\tTrial 2 Reward: -29.0\n",
      "\tTrial 3 Reward: 14.0\n",
      "\tTrial 4 Reward: 14.0\n",
      "\tTrial 5 Reward: 13.0\n",
      "\tTrial 6 Reward: 14.0\n",
      "\tTrial 7 Reward: 14.0\n",
      "\tTrial 8 Reward: 12.0\n",
      "\tTrial 9 Reward: 12.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.9, temp=0.9...\n",
      "\tTrial 0 Reward: -3.0\n",
      "\tTrial 1 Reward: -3.0\n",
      "\tTrial 2 Reward: 11.0\n",
      "\tTrial 3 Reward: 10.0\n",
      "\tTrial 4 Reward: 12.0\n",
      "\tTrial 5 Reward: -51.0\n",
      "\tTrial 6 Reward: 13.0\n",
      "\tTrial 7 Reward: 14.0\n",
      "\tTrial 8 Reward: 13.0\n",
      "\tTrial 9 Reward: -71.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.9, temp=0.99...\n",
      "\tTrial 0 Reward: -70.0\n",
      "\tTrial 1 Reward: -200.0\n",
      "\tTrial 2 Reward: -61.0\n",
      "\tTrial 3 Reward: 12.0\n",
      "\tTrial 4 Reward: 12.0\n",
      "\tTrial 5 Reward: 13.0\n",
      "\tTrial 6 Reward: 11.0\n",
      "\tTrial 7 Reward: 11.0\n",
      "\tTrial 8 Reward: 13.0\n",
      "\tTrial 9 Reward: -153.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.99, temp=0.1...\n",
      "\tTrial 0 Reward: -59.0\n",
      "\tTrial 1 Reward: -33.0\n",
      "\tTrial 2 Reward: 14.0\n",
      "\tTrial 3 Reward: 11.0\n",
      "\tTrial 4 Reward: 14.0\n",
      "\tTrial 5 Reward: 14.0\n",
      "\tTrial 6 Reward: 11.0\n",
      "\tTrial 7 Reward: 15.0\n",
      "\tTrial 8 Reward: 15.0\n",
      "\tTrial 9 Reward: 12.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.99, temp=0.5...\n",
      "\tTrial 0 Reward: -200.0\n",
      "\tTrial 1 Reward: 13.0\n",
      "\tTrial 2 Reward: 7.0\n",
      "\tTrial 3 Reward: 14.0\n",
      "\tTrial 4 Reward: 14.0\n",
      "\tTrial 5 Reward: 13.0\n",
      "\tTrial 6 Reward: 14.0\n",
      "\tTrial 7 Reward: 14.0\n",
      "\tTrial 8 Reward: 14.0\n",
      "\tTrial 9 Reward: 14.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.99, temp=0.9...\n",
      "\tTrial 0 Reward: -36.0\n",
      "\tTrial 1 Reward: -103.0\n",
      "\tTrial 2 Reward: 10.0\n",
      "\tTrial 3 Reward: 12.0\n",
      "\tTrial 4 Reward: 14.0\n",
      "\tTrial 5 Reward: 15.0\n",
      "\tTrial 6 Reward: 11.0\n",
      "\tTrial 7 Reward: 14.0\n",
      "\tTrial 8 Reward: 11.0\n",
      "\tTrial 9 Reward: 15.0\n",
      "Running experiment for sarsa with alpha=0.1, gamma=0.99, temp=0.99...\n",
      "\tTrial 0 Reward: -45.0\n",
      "\tTrial 1 Reward: 13.0\n",
      "\tTrial 2 Reward: 11.0\n",
      "\tTrial 3 Reward: 13.0\n",
      "\tTrial 4 Reward: 13.0\n",
      "\tTrial 5 Reward: 15.0\n",
      "\tTrial 6 Reward: 15.0\n",
      "\tTrial 7 Reward: 15.0\n",
      "\tTrial 8 Reward: 14.0\n",
      "\tTrial 9 Reward: 14.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.1, temp=0.1...\n",
      "\tTrial 0 Reward: -49.0\n",
      "\tTrial 1 Reward: -88.0\n",
      "\tTrial 2 Reward: -138.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -91.0\n",
      "\tTrial 5 Reward: -85.0\n",
      "\tTrial 6 Reward: -100.0\n",
      "\tTrial 7 Reward: -115.0\n",
      "\tTrial 8 Reward: -58.0\n",
      "\tTrial 9 Reward: -192.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.1, temp=0.5...\n",
      "\tTrial 0 Reward: -43.0\n",
      "\tTrial 1 Reward: -67.0\n",
      "\tTrial 2 Reward: -82.0\n",
      "\tTrial 3 Reward: -61.0\n",
      "\tTrial 4 Reward: -63.0\n",
      "\tTrial 5 Reward: -120.0\n",
      "\tTrial 6 Reward: -79.0\n",
      "\tTrial 7 Reward: -70.0\n",
      "\tTrial 8 Reward: -72.0\n",
      "\tTrial 9 Reward: -95.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.1, temp=0.9...\n",
      "\tTrial 0 Reward: -132.0\n",
      "\tTrial 1 Reward: -73.0\n",
      "\tTrial 2 Reward: -146.0\n",
      "\tTrial 3 Reward: -110.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -75.0\n",
      "\tTrial 6 Reward: -125.0\n",
      "\tTrial 7 Reward: -125.0\n",
      "\tTrial 8 Reward: -141.0\n",
      "\tTrial 9 Reward: -53.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.1, temp=0.99...\n",
      "\tTrial 0 Reward: -92.0\n",
      "\tTrial 1 Reward: -171.0\n",
      "\tTrial 2 Reward: -125.0\n",
      "\tTrial 3 Reward: -74.0\n",
      "\tTrial 4 Reward: -128.0\n",
      "\tTrial 5 Reward: -177.0\n",
      "\tTrial 6 Reward: -112.0\n",
      "\tTrial 7 Reward: -200.0\n",
      "\tTrial 8 Reward: -161.0\n",
      "\tTrial 9 Reward: -121.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.5, temp=0.1...\n",
      "\tTrial 0 Reward: -200.0\n",
      "\tTrial 1 Reward: -23.0\n",
      "\tTrial 2 Reward: -78.0\n",
      "\tTrial 3 Reward: -26.0\n",
      "\tTrial 4 Reward: -10.0\n",
      "\tTrial 5 Reward: -191.0\n",
      "\tTrial 6 Reward: 12.0\n",
      "\tTrial 7 Reward: -132.0\n",
      "\tTrial 8 Reward: -89.0\n",
      "\tTrial 9 Reward: -197.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.5, temp=0.5...\n",
      "\tTrial 0 Reward: -51.0\n",
      "\tTrial 1 Reward: -91.0\n",
      "\tTrial 2 Reward: -197.0\n",
      "\tTrial 3 Reward: -68.0\n",
      "\tTrial 4 Reward: -27.0\n",
      "\tTrial 5 Reward: -91.0\n",
      "\tTrial 6 Reward: -117.0\n",
      "\tTrial 7 Reward: -169.0\n",
      "\tTrial 8 Reward: -7.0\n",
      "\tTrial 9 Reward: -72.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.5, temp=0.9...\n",
      "\tTrial 0 Reward: -83.0\n",
      "\tTrial 1 Reward: -107.0\n",
      "\tTrial 2 Reward: -189.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -169.0\n",
      "\tTrial 5 Reward: -126.0\n",
      "\tTrial 6 Reward: -147.0\n",
      "\tTrial 7 Reward: -60.0\n",
      "\tTrial 8 Reward: -21.0\n",
      "\tTrial 9 Reward: -45.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.5, temp=0.99...\n",
      "\tTrial 0 Reward: -137.0\n",
      "\tTrial 1 Reward: -159.0\n",
      "\tTrial 2 Reward: -64.0\n",
      "\tTrial 3 Reward: -129.0\n",
      "\tTrial 4 Reward: -78.0\n",
      "\tTrial 5 Reward: -74.0\n",
      "\tTrial 6 Reward: -19.0\n",
      "\tTrial 7 Reward: -127.0\n",
      "\tTrial 8 Reward: -142.0\n",
      "\tTrial 9 Reward: -82.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.9, temp=0.1...\n",
      "\tTrial 0 Reward: -173.0\n",
      "\tTrial 1 Reward: -69.0\n",
      "\tTrial 2 Reward: -120.0\n",
      "\tTrial 3 Reward: 14.0\n",
      "\tTrial 4 Reward: -136.0\n",
      "\tTrial 5 Reward: -194.0\n",
      "\tTrial 6 Reward: -64.0\n",
      "\tTrial 7 Reward: -61.0\n",
      "\tTrial 8 Reward: -67.0\n",
      "\tTrial 9 Reward: -37.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.9, temp=0.5...\n",
      "\tTrial 0 Reward: -107.0\n",
      "\tTrial 1 Reward: -68.0\n",
      "\tTrial 2 Reward: -200.0\n",
      "\tTrial 3 Reward: -159.0\n",
      "\tTrial 4 Reward: -148.0\n",
      "\tTrial 5 Reward: -68.0\n",
      "\tTrial 6 Reward: -68.0\n",
      "\tTrial 7 Reward: -127.0\n",
      "\tTrial 8 Reward: -113.0\n",
      "\tTrial 9 Reward: -65.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.9, temp=0.9...\n",
      "\tTrial 0 Reward: -122.0\n",
      "\tTrial 1 Reward: 11.0\n",
      "\tTrial 2 Reward: 12.0\n",
      "\tTrial 3 Reward: -70.0\n",
      "\tTrial 4 Reward: -142.0\n",
      "\tTrial 5 Reward: -117.0\n",
      "\tTrial 6 Reward: -93.0\n",
      "\tTrial 7 Reward: -81.0\n",
      "\tTrial 8 Reward: -93.0\n",
      "\tTrial 9 Reward: -71.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.9, temp=0.99...\n",
      "\tTrial 0 Reward: -116.0\n",
      "\tTrial 1 Reward: 11.0\n",
      "\tTrial 2 Reward: -127.0\n",
      "\tTrial 3 Reward: -108.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -104.0\n",
      "\tTrial 6 Reward: -161.0\n",
      "\tTrial 7 Reward: -91.0\n",
      "\tTrial 8 Reward: -75.0\n",
      "\tTrial 9 Reward: -186.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.99, temp=0.1...\n",
      "\tTrial 0 Reward: -17.0\n",
      "\tTrial 1 Reward: 14.0\n",
      "\tTrial 2 Reward: 9.0\n",
      "\tTrial 3 Reward: 13.0\n",
      "\tTrial 4 Reward: 13.0\n",
      "\tTrial 5 Reward: 10.0\n",
      "\tTrial 6 Reward: 11.0\n",
      "\tTrial 7 Reward: 12.0\n",
      "\tTrial 8 Reward: 12.0\n",
      "\tTrial 9 Reward: 15.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.99, temp=0.5...\n",
      "\tTrial 0 Reward: -172.0\n",
      "\tTrial 1 Reward: 11.0\n",
      "\tTrial 2 Reward: 14.0\n",
      "\tTrial 3 Reward: 15.0\n",
      "\tTrial 4 Reward: 14.0\n",
      "\tTrial 5 Reward: 14.0\n",
      "\tTrial 6 Reward: 14.0\n",
      "\tTrial 7 Reward: 12.0\n",
      "\tTrial 8 Reward: 13.0\n",
      "\tTrial 9 Reward: 14.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.99, temp=0.9...\n",
      "\tTrial 0 Reward: -35.0\n",
      "\tTrial 1 Reward: 15.0\n",
      "\tTrial 2 Reward: 7.0\n",
      "\tTrial 3 Reward: 14.0\n",
      "\tTrial 4 Reward: 12.0\n",
      "\tTrial 5 Reward: 12.0\n",
      "\tTrial 6 Reward: 12.0\n",
      "\tTrial 7 Reward: 14.0\n",
      "\tTrial 8 Reward: 14.0\n",
      "\tTrial 9 Reward: 13.0\n",
      "Running experiment for sarsa with alpha=0.5, gamma=0.99, temp=0.99...\n",
      "\tTrial 0 Reward: -103.0\n",
      "\tTrial 1 Reward: 13.0\n",
      "\tTrial 2 Reward: 14.0\n",
      "\tTrial 3 Reward: 11.0\n",
      "\tTrial 4 Reward: 15.0\n",
      "\tTrial 5 Reward: 14.0\n",
      "\tTrial 6 Reward: 10.0\n",
      "\tTrial 7 Reward: 14.0\n",
      "\tTrial 8 Reward: 12.0\n",
      "\tTrial 9 Reward: 14.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.1, temp=0.1...\n",
      "\tTrial 0 Reward: -200.0\n",
      "\tTrial 1 Reward: -109.0\n",
      "\tTrial 2 Reward: -182.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -85.0\n",
      "\tTrial 5 Reward: -200.0\n",
      "\tTrial 6 Reward: -137.0\n",
      "\tTrial 7 Reward: -153.0\n",
      "\tTrial 8 Reward: -74.0\n",
      "\tTrial 9 Reward: -11.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.1, temp=0.5...\n",
      "\tTrial 0 Reward: -65.0\n",
      "\tTrial 1 Reward: -200.0\n",
      "\tTrial 2 Reward: -103.0\n",
      "\tTrial 3 Reward: -150.0\n",
      "\tTrial 4 Reward: -87.0\n",
      "\tTrial 5 Reward: -53.0\n",
      "\tTrial 6 Reward: -200.0\n",
      "\tTrial 7 Reward: -61.0\n",
      "\tTrial 8 Reward: -91.0\n",
      "\tTrial 9 Reward: -133.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.1, temp=0.9...\n",
      "\tTrial 0 Reward: -72.0\n",
      "\tTrial 1 Reward: -69.0\n",
      "\tTrial 2 Reward: -187.0\n",
      "\tTrial 3 Reward: -81.0\n",
      "\tTrial 4 Reward: -143.0\n",
      "\tTrial 5 Reward: -47.0\n",
      "\tTrial 6 Reward: -152.0\n",
      "\tTrial 7 Reward: -72.0\n",
      "\tTrial 8 Reward: -66.0\n",
      "\tTrial 9 Reward: -48.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.1, temp=0.99...\n",
      "\tTrial 0 Reward: -156.0\n",
      "\tTrial 1 Reward: -110.0\n",
      "\tTrial 2 Reward: -89.0\n",
      "\tTrial 3 Reward: -149.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -141.0\n",
      "\tTrial 6 Reward: -44.0\n",
      "\tTrial 7 Reward: -115.0\n",
      "\tTrial 8 Reward: -129.0\n",
      "\tTrial 9 Reward: -200.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.5, temp=0.1...\n",
      "\tTrial 0 Reward: -101.0\n",
      "\tTrial 1 Reward: -147.0\n",
      "\tTrial 2 Reward: -52.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -38.0\n",
      "\tTrial 5 Reward: -94.0\n",
      "\tTrial 6 Reward: -200.0\n",
      "\tTrial 7 Reward: -49.0\n",
      "\tTrial 8 Reward: -200.0\n",
      "\tTrial 9 Reward: -56.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.5, temp=0.5...\n",
      "\tTrial 0 Reward: -200.0\n",
      "\tTrial 1 Reward: -123.0\n",
      "\tTrial 2 Reward: -186.0\n",
      "\tTrial 3 Reward: -102.0\n",
      "\tTrial 4 Reward: -158.0\n",
      "\tTrial 5 Reward: -200.0\n",
      "\tTrial 6 Reward: -85.0\n",
      "\tTrial 7 Reward: -126.0\n",
      "\tTrial 8 Reward: -83.0\n",
      "\tTrial 9 Reward: -87.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.5, temp=0.9...\n",
      "\tTrial 0 Reward: -186.0\n",
      "\tTrial 1 Reward: -40.0\n",
      "\tTrial 2 Reward: -92.0\n",
      "\tTrial 3 Reward: -68.0\n",
      "\tTrial 4 Reward: -96.0\n",
      "\tTrial 5 Reward: -73.0\n",
      "\tTrial 6 Reward: -126.0\n",
      "\tTrial 7 Reward: -35.0\n",
      "\tTrial 8 Reward: -141.0\n",
      "\tTrial 9 Reward: -200.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.5, temp=0.99...\n",
      "\tTrial 0 Reward: -59.0\n",
      "\tTrial 1 Reward: -153.0\n",
      "\tTrial 2 Reward: -180.0\n",
      "\tTrial 3 Reward: -12.0\n",
      "\tTrial 4 Reward: -152.0\n",
      "\tTrial 5 Reward: -78.0\n",
      "\tTrial 6 Reward: -40.0\n",
      "\tTrial 7 Reward: -135.0\n",
      "\tTrial 8 Reward: -65.0\n",
      "\tTrial 9 Reward: -126.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.9, temp=0.1...\n",
      "\tTrial 0 Reward: -150.0\n",
      "\tTrial 1 Reward: -200.0\n",
      "\tTrial 2 Reward: -55.0\n",
      "\tTrial 3 Reward: -128.0\n",
      "\tTrial 4 Reward: -50.0\n",
      "\tTrial 5 Reward: -84.0\n",
      "\tTrial 6 Reward: -22.0\n",
      "\tTrial 7 Reward: -200.0\n",
      "\tTrial 8 Reward: -67.0\n",
      "\tTrial 9 Reward: -28.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.9, temp=0.5...\n",
      "\tTrial 0 Reward: -200.0\n",
      "\tTrial 1 Reward: -118.0\n",
      "\tTrial 2 Reward: -119.0\n",
      "\tTrial 3 Reward: -50.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -26.0\n",
      "\tTrial 6 Reward: -130.0\n",
      "\tTrial 7 Reward: -104.0\n",
      "\tTrial 8 Reward: -200.0\n",
      "\tTrial 9 Reward: -61.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.9, temp=0.9...\n",
      "\tTrial 0 Reward: -99.0\n",
      "\tTrial 1 Reward: -61.0\n",
      "\tTrial 2 Reward: -92.0\n",
      "\tTrial 3 Reward: -97.0\n",
      "\tTrial 4 Reward: -36.0\n",
      "\tTrial 5 Reward: -71.0\n",
      "\tTrial 6 Reward: -59.0\n",
      "\tTrial 7 Reward: -141.0\n",
      "\tTrial 8 Reward: -130.0\n",
      "\tTrial 9 Reward: -50.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.9, temp=0.99...\n",
      "\tTrial 0 Reward: -127.0\n",
      "\tTrial 1 Reward: -62.0\n",
      "\tTrial 2 Reward: -27.0\n",
      "\tTrial 3 Reward: -91.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -30.0\n",
      "\tTrial 6 Reward: -94.0\n",
      "\tTrial 7 Reward: -66.0\n",
      "\tTrial 8 Reward: 15.0\n",
      "\tTrial 9 Reward: -136.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.99, temp=0.1...\n",
      "\tTrial 0 Reward: 14.0\n",
      "\tTrial 1 Reward: -68.0\n",
      "\tTrial 2 Reward: -63.0\n",
      "\tTrial 3 Reward: -48.0\n",
      "\tTrial 4 Reward: -97.0\n",
      "\tTrial 5 Reward: -45.0\n",
      "\tTrial 6 Reward: -49.0\n",
      "\tTrial 7 Reward: -52.0\n",
      "\tTrial 8 Reward: -120.0\n",
      "\tTrial 9 Reward: -200.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.99, temp=0.5...\n",
      "\tTrial 0 Reward: -186.0\n",
      "\tTrial 1 Reward: -84.0\n",
      "\tTrial 2 Reward: -63.0\n",
      "\tTrial 3 Reward: -90.0\n",
      "\tTrial 4 Reward: -92.0\n",
      "\tTrial 5 Reward: -31.0\n",
      "\tTrial 6 Reward: -200.0\n",
      "\tTrial 7 Reward: -171.0\n",
      "\tTrial 8 Reward: -101.0\n",
      "\tTrial 9 Reward: -134.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.99, temp=0.9...\n",
      "\tTrial 0 Reward: -163.0\n",
      "\tTrial 1 Reward: -113.0\n",
      "\tTrial 2 Reward: -200.0\n",
      "\tTrial 3 Reward: -33.0\n",
      "\tTrial 4 Reward: -160.0\n",
      "\tTrial 5 Reward: -156.0\n",
      "\tTrial 6 Reward: -30.0\n",
      "\tTrial 7 Reward: -153.0\n",
      "\tTrial 8 Reward: -94.0\n",
      "\tTrial 9 Reward: -134.0\n",
      "Running experiment for sarsa with alpha=0.9, gamma=0.99, temp=0.99...\n",
      "\tTrial 0 Reward: -70.0\n",
      "\tTrial 1 Reward: 13.0\n",
      "\tTrial 2 Reward: -200.0\n",
      "\tTrial 3 Reward: -95.0\n",
      "\tTrial 4 Reward: 15.0\n",
      "\tTrial 5 Reward: -79.0\n",
      "\tTrial 6 Reward: -100.0\n",
      "\tTrial 7 Reward: -129.0\n",
      "\tTrial 8 Reward: -200.0\n",
      "\tTrial 9 Reward: -29.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.1, temp=0.1...\n",
      "\tTrial 0 Reward: -27.0\n",
      "\tTrial 1 Reward: -120.0\n",
      "\tTrial 2 Reward: -151.0\n",
      "\tTrial 3 Reward: -129.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -188.0\n",
      "\tTrial 6 Reward: -48.0\n",
      "\tTrial 7 Reward: -108.0\n",
      "\tTrial 8 Reward: -80.0\n",
      "\tTrial 9 Reward: -104.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.1, temp=0.5...\n",
      "\tTrial 0 Reward: -83.0\n",
      "\tTrial 1 Reward: -31.0\n",
      "\tTrial 2 Reward: -147.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -141.0\n",
      "\tTrial 5 Reward: -134.0\n",
      "\tTrial 6 Reward: -200.0\n",
      "\tTrial 7 Reward: -141.0\n",
      "\tTrial 8 Reward: -88.0\n",
      "\tTrial 9 Reward: -118.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.1, temp=0.9...\n",
      "\tTrial 0 Reward: -79.0\n",
      "\tTrial 1 Reward: -119.0\n",
      "\tTrial 2 Reward: -108.0\n",
      "\tTrial 3 Reward: -175.0\n",
      "\tTrial 4 Reward: -150.0\n",
      "\tTrial 5 Reward: -60.0\n",
      "\tTrial 6 Reward: -77.0\n",
      "\tTrial 7 Reward: -62.0\n",
      "\tTrial 8 Reward: -97.0\n",
      "\tTrial 9 Reward: -60.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.1, temp=0.99...\n",
      "\tTrial 0 Reward: -115.0\n",
      "\tTrial 1 Reward: -29.0\n",
      "\tTrial 2 Reward: -140.0\n",
      "\tTrial 3 Reward: -147.0\n",
      "\tTrial 4 Reward: -55.0\n",
      "\tTrial 5 Reward: -200.0\n",
      "\tTrial 6 Reward: -158.0\n",
      "\tTrial 7 Reward: -37.0\n",
      "\tTrial 8 Reward: -75.0\n",
      "\tTrial 9 Reward: -65.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.5, temp=0.1...\n",
      "\tTrial 0 Reward: -179.0\n",
      "\tTrial 1 Reward: -114.0\n",
      "\tTrial 2 Reward: -200.0\n",
      "\tTrial 3 Reward: -194.0\n",
      "\tTrial 4 Reward: -37.0\n",
      "\tTrial 5 Reward: -66.0\n",
      "\tTrial 6 Reward: -27.0\n",
      "\tTrial 7 Reward: -200.0\n",
      "\tTrial 8 Reward: -107.0\n",
      "\tTrial 9 Reward: -175.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.5, temp=0.5...\n",
      "\tTrial 0 Reward: -53.0\n",
      "\tTrial 1 Reward: -110.0\n",
      "\tTrial 2 Reward: -91.0\n",
      "\tTrial 3 Reward: -91.0\n",
      "\tTrial 4 Reward: -84.0\n",
      "\tTrial 5 Reward: -164.0\n",
      "\tTrial 6 Reward: -132.0\n",
      "\tTrial 7 Reward: -15.0\n",
      "\tTrial 8 Reward: -98.0\n",
      "\tTrial 9 Reward: -194.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.5, temp=0.9...\n",
      "\tTrial 0 Reward: -102.0\n",
      "\tTrial 1 Reward: -30.0\n",
      "\tTrial 2 Reward: -25.0\n",
      "\tTrial 3 Reward: -58.0\n",
      "\tTrial 4 Reward: -70.0\n",
      "\tTrial 5 Reward: -32.0\n",
      "\tTrial 6 Reward: -99.0\n",
      "\tTrial 7 Reward: -127.0\n",
      "\tTrial 8 Reward: -153.0\n",
      "\tTrial 9 Reward: -119.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.5, temp=0.99...\n",
      "\tTrial 0 Reward: -57.0\n",
      "\tTrial 1 Reward: -200.0\n",
      "\tTrial 2 Reward: -116.0\n",
      "\tTrial 3 Reward: -200.0\n",
      "\tTrial 4 Reward: -63.0\n",
      "\tTrial 5 Reward: -200.0\n",
      "\tTrial 6 Reward: -163.0\n",
      "\tTrial 7 Reward: -71.0\n",
      "\tTrial 8 Reward: -44.0\n",
      "\tTrial 9 Reward: -126.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.9, temp=0.1...\n",
      "\tTrial 0 Reward: -156.0\n",
      "\tTrial 1 Reward: -64.0\n",
      "\tTrial 2 Reward: -185.0\n",
      "\tTrial 3 Reward: -71.0\n",
      "\tTrial 4 Reward: -119.0\n",
      "\tTrial 5 Reward: -132.0\n",
      "\tTrial 6 Reward: -125.0\n",
      "\tTrial 7 Reward: -72.0\n",
      "\tTrial 8 Reward: -124.0\n",
      "\tTrial 9 Reward: -115.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.9, temp=0.5...\n",
      "\tTrial 0 Reward: -194.0\n",
      "\tTrial 1 Reward: -104.0\n",
      "\tTrial 2 Reward: 9.0\n",
      "\tTrial 3 Reward: -87.0\n",
      "\tTrial 4 Reward: -93.0\n",
      "\tTrial 5 Reward: -83.0\n",
      "\tTrial 6 Reward: -78.0\n",
      "\tTrial 7 Reward: -80.0\n",
      "\tTrial 8 Reward: -60.0\n",
      "\tTrial 9 Reward: -60.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.9, temp=0.9...\n",
      "\tTrial 0 Reward: -200.0\n",
      "\tTrial 1 Reward: -109.0\n",
      "\tTrial 2 Reward: -184.0\n",
      "\tTrial 3 Reward: -56.0\n",
      "\tTrial 4 Reward: -9.0\n",
      "\tTrial 5 Reward: -100.0\n",
      "\tTrial 6 Reward: -98.0\n",
      "\tTrial 7 Reward: -188.0\n",
      "\tTrial 8 Reward: -110.0\n",
      "\tTrial 9 Reward: -112.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.9, temp=0.99...\n",
      "\tTrial 0 Reward: -151.0\n",
      "\tTrial 1 Reward: -200.0\n",
      "\tTrial 2 Reward: -200.0\n",
      "\tTrial 3 Reward: -85.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -3.0\n",
      "\tTrial 6 Reward: -142.0\n",
      "\tTrial 7 Reward: -169.0\n",
      "\tTrial 8 Reward: -200.0\n",
      "\tTrial 9 Reward: -119.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.99, temp=0.1...\n",
      "\tTrial 0 Reward: -47.0\n",
      "\tTrial 1 Reward: -72.0\n",
      "\tTrial 2 Reward: -101.0\n",
      "\tTrial 3 Reward: -120.0\n",
      "\tTrial 4 Reward: -200.0\n",
      "\tTrial 5 Reward: -113.0\n",
      "\tTrial 6 Reward: -67.0\n",
      "\tTrial 7 Reward: -140.0\n",
      "\tTrial 8 Reward: -63.0\n",
      "\tTrial 9 Reward: -65.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.99, temp=0.5...\n",
      "\tTrial 0 Reward: -154.0\n",
      "\tTrial 1 Reward: -122.0\n",
      "\tTrial 2 Reward: -124.0\n",
      "\tTrial 3 Reward: -88.0\n",
      "\tTrial 4 Reward: -129.0\n",
      "\tTrial 5 Reward: -156.0\n",
      "\tTrial 6 Reward: -83.0\n",
      "\tTrial 7 Reward: -200.0\n",
      "\tTrial 8 Reward: -184.0\n",
      "\tTrial 9 Reward: -73.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.99, temp=0.9...\n",
      "\tTrial 0 Reward: -39.0\n",
      "\tTrial 1 Reward: -128.0\n",
      "\tTrial 2 Reward: -41.0\n",
      "\tTrial 3 Reward: -16.0\n",
      "\tTrial 4 Reward: -125.0\n",
      "\tTrial 5 Reward: -148.0\n",
      "\tTrial 6 Reward: -154.0\n",
      "\tTrial 7 Reward: -42.0\n",
      "\tTrial 8 Reward: -86.0\n",
      "\tTrial 9 Reward: -16.0\n",
      "Running experiment for sarsa with alpha=0.99, gamma=0.99, temp=0.99...\n",
      "\tTrial 0 Reward: -42.0\n",
      "\tTrial 1 Reward: -163.0\n",
      "\tTrial 2 Reward: -200.0\n",
      "\tTrial 3 Reward: -56.0\n",
      "\tTrial 4 Reward: -93.0\n",
      "\tTrial 5 Reward: -42.0\n",
      "\tTrial 6 Reward: -165.0\n",
      "\tTrial 7 Reward: -175.0\n",
      "\tTrial 8 Reward: -156.0\n",
      "\tTrial 9 Reward: -98.0\n",
      "Running experiment for expected sarsa with alpha=0.1, gamma=0.1, temp=0.1...Reward: [-767. -866. -740. -713. -920. -695. -866. -794. -749. -803. -200. -659.\n",
      " -875. -215. -695. -776. -695. -812. -740. -713. -200. -677. -767. -794.\n",
      " -581. -179. -713. -632. -758. -695. -200. -920. -722. -803. -646. -794.\n",
      " -866. -848. -794. -875. -200. -785. -821. -830. -758. -731. -776. -884.\n",
      " -767. -567. -200. -812. -875. -776. -767. -731. -812. -893. -938. -893.\n",
      " -200. -857. -749. -722. -929. -749. -902. -749. -794. -731. -200. -785.\n",
      " -713. -767. -661. -660. -749. -884. -821. -839. -200. -740. -776. -722.\n",
      " -956. -740. -803. -713. -794. -848. -200. -659. -767. -758. -704. -848.\n",
      " -812. -686. -830. -812. -200. -821. -695. -758. -866. -767. -731. -794.\n",
      " -722. -785. -200. -830. -839. -839. -767. -713. -839. -821. -803. -528.\n",
      " -200. -731. -929. -731. -884. -884. -776. -722. -803. -228. -200. -785.\n",
      " -812. -830. -857. -821. -767. -731. -875. -830. -200. -767. -884. -866.\n",
      " -758. -812. -749. -758. -713. -848. -200. -776. -794. -695. -803. -821.\n",
      " -767. -794. -686. -749. -200. -776. -821. -673. -839. -866. -785. -704.\n",
      " -657. -776. -200. -857. -169. -848. -857. -632. -803. -794. -848. -812.\n",
      " -200. -866. -866. -866. -158. -848. -749. -848. -647. -740. -200. -893.\n",
      " -731. -776. -713. -785. -875. -686. -794. -812. -200. -749. -875. -830.\n",
      " -626. -722. -785. -803. -821. -839. -200. -830. -794. -785. -866. -749.\n",
      " -677. -785. -785. -740. -200. -758. -830. -722. -893. -731. -803. -758.\n",
      " -695. -758. -200. -749. -740. -166. -767. -722. -785. -785. -722. -451.\n",
      " -200. -866. -785. -776. -785. -785. -740. -848. -758. -776. -200. -785.\n",
      " -740. -830. -830. -848. -839. -776. -677. -695. -200. -920. -767. -803.\n",
      " -830. -839. -839. -839. -794. -740. -200. -803. -812. -848. -722. -722.\n",
      " -821. -803. -803. -794. -200. -776.  -76. -749. -722. -704. -857. -857.\n",
      " -785. -740. -200. -695. -776. -821. -821. -776. -812. -749. -749. -830.\n",
      " -200. -785. -821. -722. -767. -758. -893. -902. -857. -749. -200. -713.\n",
      " -776. -812. -758. -758. -713. -884. -812. -740. -200. -794. -722. -866.\n",
      " -704. -794. -830. -803. -776. -902. -200. -713. -731. -794. -875. -722.\n",
      " -794. -749. -794. -875. -200. -731. -794. -722. -839. -830. -812. -731.\n",
      " -911. -776. -200. -749. -776. -677. -785. -699. -776. -785. -803. -776.\n",
      " -200. -839. -785. -830. -695. -803. -839. -776. -785. -767. -200. -803.\n",
      " -677. -839. -812. -803. -704. -785. -731. -758. -200. -821. -722. -776.\n",
      " -794. -785. -803. -794. -785. -812. -200. -695. -722. -767. -731. -830.\n",
      " -740. -848. -821. -704. -200. -857. -767. -821. -848. -776. -803. -731.\n",
      " -740. -776. -200. -731. -260. -857. -830. -776. -767. -803. -749. -785.\n",
      " -200. -767. -794. -812. -902. -677. -902. -794. -812. -731. -200. -704.\n",
      " -785. -722. -794. -776. -767. -830. -722. -776. -200. -785. -821. -758.\n",
      " -695. -722. -839. -812. -776. -686. -200. -902. -794. -857. -812. -614.\n",
      " -857. -866. -758. -659. -200. -731. -749. -776. -812. -767. -830. -767.\n",
      " -830. -857. -200. -812. -740. -686. -893. -866. -749. -857. -812. -785.\n",
      " -200. -884. -812. -731. -713. -704. -776. -767. -830. -821. -200. -794.\n",
      " -803. -632. -794. -803. -821. -803. -839. -767.]\n",
      "Running experiment for expected sarsa with alpha=0.1, gamma=0.1, temp=0.5...Reward: [-758. -704. -776. -776. -821. -767. -812. -758. -668. -722. -200. -276.\n",
      " -740. -486. -803. -776. -821. -803. -767. -686. -200. -803. -839. -695.\n",
      " -767. -740. -776. -785. -803. -722. -200. -857. -794. -767. -731. -857.\n",
      " -623. -911. -713. -812. -200. -857. -785. -803. -884. -794. -830. -650.\n",
      " -857. -393. -200. -767. -767. -803. -803. -695. -830. -704. -884. -794.\n",
      " -200. -893. -740. -659. -785. -821. -767. -722. -668. -776. -200. -839.\n",
      " -623. -857. -776. -812. -803. -722. -911. -598. -200. -794. -398. -749.\n",
      " -785. -731. -731. -776. -767. -758. -200. -830. -983. -758. -794. -722.\n",
      " -758. -794. -821. -758. -200. -767. -812. -722. -803. -821. -713. -866.\n",
      " -731. -776. -200. -767. -848. -857. -839. -812. -517. -731. -776. -749.\n",
      " -200. -830. -857. -830. -839. -785. -740. -875. -695. -758. -200. -713.\n",
      " -332. -722. -731. -839. -839. -875. -821. -749. -200. -740. -803. -758.\n",
      " -686. -722. -704. -893. -848. -839. -200. -794. -740. -740. -749. -722.\n",
      " -812. -839. -731. -803. -200. -758. -821. -794. -803. -812. -740. -713.\n",
      " -767. -749. -200. -821. -812. -812. -758. -812. -758. -758.  -98. -857.\n",
      " -200. -767. -731. -785. -686. -197. -839. -740. -785. -740. -200. -722.\n",
      " -803. -875. -596. -848. -794. -812. -776. -839. -200. -920. -965. -722.\n",
      " -767. -686. -857. -857. -803. -848. -200. -767. -731. -803. -830. -857.\n",
      " -767. -902. -731. -767. -200. -794. -776. -794. -668. -713. -731. -704.\n",
      " -776. -857. -200. -785. -866. -758. -767. -794. -695. -740. -767. -794.\n",
      " -200. -875. -848. -785. -776. -740. -758. -731. -731. -713. -200. -704.\n",
      " -866. -803. -866. -803. -830. -893. -812. -785. -200. -866. -713. -794.\n",
      " -686. -758. -911. -749. -812. -830. -200. -767. -803. -848. -776. -812.\n",
      " -677. -668. -803. -821. -200. -830. -659. -839. -445. -875. -695. -803.\n",
      " -758. -866. -200. -938. -785. -740. -668. -713. -857. -785. -785. -776.\n",
      " -200. -758. -875. -731. -785. -785. -794. -839. -785. -614. -200. -722.\n",
      " -677. -758. -812. -794. -839. -875. -812. -686. -200. -857. -821. -618.\n",
      " -731. -785. -731. -785. -713. -686. -200. -695. -740. -866. -329. -767.\n",
      " -812. -848. -758. -700. -200. -785. -758. -785. -866. -857. -386. -722.\n",
      " -785. -866. -200. -507. -722. -704. -659. -785. -740. -740. -695. -803.\n",
      " -200. -776. -812. -713. -767. -794. -866. -893. -866. -704. -200. -785.\n",
      " -830. -695. -830. -794. -794. -776. -794. -812. -200. -749. -767. -731.\n",
      " -785. -803. -866. -776. -794. -848. -200. -839. -821. -641. -668. -821.\n",
      " -722. -758. -911. -715. -200. -821. -767. -929. -758. -893. -776. -857.\n",
      " -812. -767. -200. -776. -758. -794. -848. -857. -866. -776. -668. -785.\n",
      " -200. -839. -848. -794. -758. -776. -722. -731. -785. -758. -200. -740.\n",
      " -749. -857. -632. -749. -821. -812. -664. -857. -200. -830. -830. -677.\n",
      " -740. -749. -812. -794. -866. -794. -200. -848. -776. -686. -857. -821.\n",
      " -776. -920. -740. -722. -200. -848. -650. -478. -794. -731. -794. -758.\n",
      " -794. -794. -200. -527. -839. -821. -803. -758. -686. -821. -821. -875.\n",
      " -200. -821. -686. -749. -722. -875. -767. -875. -731. -391. -200. -677.\n",
      " -794. -821. -776. -686. -767. -713. -758. -902.]\n",
      "Running experiment for expected sarsa with alpha=0.1, gamma=0.1, temp=0.9...Reward: [-749. -803. -830. -803. -722. -794. -713. -767. -758. -812. -200. -776.\n",
      " -731. -530. -617. -875. -767. -830. -821. -704. -200. -794. -776. -767.\n",
      " -785. -794. -731. -848. -947. -812. -200. -821. -785. -731. -785. -821.\n",
      " -731. -767. -812. -767. -200. -884. -848. -839. -731. -803. -839. -884.\n",
      " -830. -884. -200. -659. -758. -739. -722. -767. -830. -722. -776. -776.\n",
      " -200. -902. -695. -794. -758. -830. -830. -857. -785. -740. -200. -839.\n",
      " -776. -564. -812. -794. -821. -911. -332. -767. -200. -677. -839. -749.\n",
      " -222. -776. -677. -749. -821. -848. -200. -920. -830. -839. -758. -731.\n",
      " -650. -794. -697. -686. -200. -749. -830. -794. -180. -857. -731. -794.\n",
      " -740. -857. -200. -776. -722. -920. -857. -713. -704. -749. -848. -830.\n",
      " -200. -830. -731. -502. -767. -758. -884. -686. -757. -758. -200. -785.\n",
      " -650. -776. -848. -740. -731. -758. -875. -767. -200. -740. -812. -767.\n",
      " -848. -749. -466. -803. -785. -713. -200. -803. -785. -556. -821. -695.\n",
      " -127. -785. -767. -839. -200. -785. -463. -875. -812. -740. -803. -821.\n",
      " -749. -821. -200. -794. -785. -794. -722. -803. -749. -776. -659. -803.\n",
      " -200. -713. -866. -839. -749. -614. -534. -785. -794. -875. -200. -866.\n",
      " -830. -758. -821. -713. -875. -839. -803. -857. -200. -767. -794. -803.\n",
      " -821. -704. -767. -713. -776. -776. -200. -776. -803. -713. -695. -749.\n",
      " -695. -704. -722. -668. -200. -713. -740. -677. -776. -857. -767. -848.\n",
      " -803. -668. -200. -830. -875. -767. -848. -803. -767. -839. -668. -857.\n",
      " -200. -632. -812. -785. -596. -821. -677. -758. -713. -830. -200. -794.\n",
      " -659. -767. -722. -830. -857. -695. -857. -812. -200. -767. -776. -740.\n",
      " -776. -866. -704. -875. -740. -803. -200. -767. -875. -956. -767. -776.\n",
      " -758. -839. -686. -704. -200. -758. -794. -731. -911. -695. -704. -812.\n",
      " -812. -776. -200. -713. -785. -731. -713. -704. -839. -920. -803. -713.\n",
      " -200. -652. -803. -731. -722. -749. -866. -884. -875. -758. -200. -767.\n",
      " -803. -758. -794. -685. -839. -722. -812. -848. -200. -740. -884. -767.\n",
      " -767. -830. -839. -650. -803. -776. -200. -875. -749. -740. -776. -794.\n",
      " -803. -803. -857. -848. -200. -812. -526. -230. -812. -704. -704. -794.\n",
      " -821. -767. -200. -713. -722. -857. -812. -713. -722. -641. -857. -559.\n",
      " -200. -821. -565. -821. -758. -920. -704. -785. -740. -821. -200. -929.\n",
      " -821. -776. -767. -785. -767. -803. -794. -803. -200. -760. -776. -884.\n",
      " -695. -740. -830. -794. -821. -884. -200. -866. -731. -758. -704. -785.\n",
      " -803. -866. -758. -848. -200. -785. -758. -794. -848. -830. -749. -695.\n",
      " -830. -848. -200. -758. -848. -470. -821. -740. -812. -830. -902. -902.\n",
      " -200. -758. -821. -875. -821. -875. -812. -893. -776. -695. -200. -722.\n",
      " -749. -713. -722. -740. -821. -812. -713. -767. -200. -731. -713. -740.\n",
      " -821. -803. -830. -821. -821. -920. -200. -812. -803. -812. -776. -875.\n",
      " -767. -767. -776. -767. -200. -875. -713. -776. -821. -722. -776. -875.\n",
      " -812. -848. -200. -767. -713. -695. -857. -713. -758. -866. -776. -740.\n",
      " -200. -677. -749. -830. -812. -803. -713. -803. -911. -785. -200. -740.\n",
      " -875. -839. -176. -776. -821. -866. -758. -695.]\n",
      "Running experiment for expected sarsa with alpha=0.1, gamma=0.1, temp=0.99..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[98], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m expected_sarsa_agents:\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning experiment for expected sarsa with alpha=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00magent\u001B[38;5;241m.\u001B[39malpha\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, gamma=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00magent\u001B[38;5;241m.\u001B[39mgamma\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, temp=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00magent\u001B[38;5;241m.\u001B[39mtemp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m, flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 28\u001B[0m     expected_sarsa_rewards[alphas\u001B[38;5;241m.\u001B[39mindex(agent\u001B[38;5;241m.\u001B[39malpha), gammas\u001B[38;5;241m.\u001B[39mindex(agent\u001B[38;5;241m.\u001B[39mgamma), temps\u001B[38;5;241m.\u001B[39mindex(agent\u001B[38;5;241m.\u001B[39mtemp)] \u001B[38;5;241m=\u001B[39m \u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_segments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReward: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_sarsa_rewards[alphas\u001B[38;5;241m.\u001B[39mindex(agent\u001B[38;5;241m.\u001B[39malpha),\u001B[38;5;250m \u001B[39mgammas\u001B[38;5;241m.\u001B[39mindex(agent\u001B[38;5;241m.\u001B[39mgamma),\u001B[38;5;250m \u001B[39mtemps\u001B[38;5;241m.\u001B[39mindex(agent\u001B[38;5;241m.\u001B[39mtemp),\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[91], line 26\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(agent, env, num_segments, max_steps)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_segments):\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m i \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 26\u001B[0m         rewards[i] \u001B[38;5;241m=\u001B[39m \u001B[43mrun_episode\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     28\u001B[0m         rewards[i] \u001B[38;5;241m=\u001B[39m run_episode(agent, env, max_steps, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[91], line 8\u001B[0m, in \u001B[0;36mrun_episode\u001B[0;34m(agent, env, max_steps, train)\u001B[0m\n\u001B[1;32m      6\u001B[0m a \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mselect_action(s, \u001B[38;5;129;01mnot\u001B[39;00m train)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_steps):\n\u001B[0;32m----> 8\u001B[0m     s_prime, r, done, info, mask \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     a_prime \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mselect_action(s_prime, \u001B[38;5;129;01mnot\u001B[39;00m train)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m train:\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/McGill Winter 2024/COMP 579/COMP579/venv/lib/python3.11/site-packages/gym/wrappers/time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/McGill Winter 2024/COMP 579/COMP579/venv/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/McGill Winter 2024/COMP 579/COMP579/venv/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/McGill Winter 2024/COMP 579/COMP579/venv/lib/python3.11/site-packages/gym/envs/toy_text/taxi.py:263\u001B[0m, in \u001B[0;36mTaxiEnv.step\u001B[0;34m(self, a)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender()\n\u001B[0;32m--> 263\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28mint\u001B[39m(s), r, t, \u001B[38;5;28;01mFalse\u001B[39;00m, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprob\u001B[39m\u001B[38;5;124m\"\u001B[39m: p, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maction_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m})\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/McGill Winter 2024/COMP 579/COMP579/venv/lib/python3.11/site-packages/gym/envs/toy_text/taxi.py:235\u001B[0m, in \u001B[0;36mTaxiEnv.action_mask\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maction_mask\u001B[39m(\u001B[38;5;28mself\u001B[39m, state: \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    234\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Computes an action mask for the action space using the state information.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 235\u001B[0m     mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m6\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint8)\n\u001B[1;32m    236\u001B[0m     taxi_row, taxi_col, pass_loc, dest_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode(state)\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m taxi_row \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# define hyperparameter arrays\n",
    "num_segments = 500\n",
    "max_steps = 200\n",
    "alphas = [0.1, 0.5, 0.9, 0.99]\n",
    "gammas = [0.1, 0.5, 0.9, 0.99]\n",
    "temps = [0.1, 0.5, 0.9, 0.99]\n",
    "num_trials = 10\n",
    "\n",
    "# define agents\n",
    "sarsa_agents = [Sarsa(env, alpha, gamma, temp) for alpha in alphas for gamma in gammas for temp in temps]\n",
    "expected_sarsa_agents = [ExpectedSarsa(env, alpha, gamma, temp) for alpha in alphas for gamma in gammas for temp in temps]\n",
    "\n",
    "# define result arrays with uncertainty\n",
    "sarsa_rewards = np.zeros((num_trials, len(alphas), len(gammas), len(temps), num_segments))\n",
    "expected_sarsa_rewards = np.zeros((num_trials, len(alphas), len(gammas), len(temps), num_segments))\n",
    "\n",
    "# run experiments\n",
    "for agent in sarsa_agents:\n",
    "    print(f\"Running experiment for sarsa with alpha={agent.alpha}, gamma={agent.gamma}, temp={agent.temp}...\")\n",
    "    for trial in range(num_trials):\n",
    "        sarsa_rewards[trial, alphas.index(agent.alpha), gammas.index(agent.gamma), temps.index(agent.temp)] = run_experiment(agent, env, num_segments, max_steps)\n",
    "        print(f\"\\tTrial {trial} Reward: {sarsa_rewards[trial, alphas.index(agent.alpha), gammas.index(agent.gamma), temps.index(agent.temp), :].max()}\")\n",
    "    \n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T06:01:18.131182Z",
     "start_time": "2024-02-19T05:53:31.724553Z"
    }
   },
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-19T05:45:25.722928Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-19T05:45:25.724073Z"
    }
   }
  }
 ]
}
