{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb514261acb7de6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# COMP 579: Assignment 3\n",
    "March 19th, 2024\n",
    "Group 86:\n",
    "- Mathieu Geoffroy 260986559\n",
    "- Ryan Reszetnik 260948454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629507c78c8a34ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Value-based methods with linear function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a3f3021fcb387",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:17:09.261080Z",
     "start_time": "2024-03-13T16:17:08.689152Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923762f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tile encoding of the state space to ma\n",
    "def create_tiling_for_feature(low,high,num_tiles_per_tiling,num_tilings,x):\n",
    "    # print(low,high,num_tiles_per_tiling,num_tilings,x)#-1.2 0.6 2 8 [-0.5906537  0.       ]\n",
    "    num_deltas = (num_tiles_per_tiling-1)*num_tilings+1\n",
    "    tiling = np.linspace(low,high,num_deltas)\n",
    "    #find the index that x falls into\n",
    "    idx = np.digitize(x,tiling)\n",
    "    #idx is 1 indexed\n",
    "    idx = idx - 1\n",
    "\n",
    "    representation = np.zeros((num_tilings,num_tiles_per_tiling))\n",
    "    for i in range(num_tilings):\n",
    "        one_hot = (idx+num_tilings-i-1) // num_tiles_per_tiling\n",
    "        #convert to one hot encoding\n",
    "        # print(one_hot,num_tilings,i,idx,num_tiles_per_tiling)#4 8 0 2 2\n",
    "        representation[i][one_hot] = 1\n",
    "    return representation\n",
    "def create_tiling(env,num_tiles_per_tiling,num_tilings,state):\n",
    "    low = env.observation_space.low\n",
    "    high = env.observation_space.high\n",
    "    num_features = len(low)\n",
    "    tiling = np.zeros((num_features,num_tilings,num_tiles_per_tiling))\n",
    "    for i in range(num_features):\n",
    "        tiling[i] = create_tiling_for_feature(low[i],high[i],num_tiles_per_tiling,num_tilings,state[i])\n",
    "    return tiling\n",
    "def create_flat_tiling(env,num_tiles_per_tiling,num_tilings,state):\n",
    "    tiling = create_tiling(env,num_tiles_per_tiling,num_tilings,state)\n",
    "    flat_tiling = tiling.flatten()\n",
    "    return flat_tiling\n",
    "#test the tiling\n",
    "mtn_car = gym.make('MountainCar-v0')\n",
    "state = mtn_car.reset()\n",
    "print(state)\n",
    "print(create_tiling(mtn_car,8,2,state[0]))\n",
    "tiling = create_flat_tiling(mtn_car,8,2,state[0])\n",
    "print(tiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.uniform(-0.001, 0.001, (input_dim, output_dim))\n",
    "    def forward(self, x):\n",
    "        return x @ self.weights\n",
    "    def backward(self, x, y_hat, lr):\n",
    "        self.weights = self.weights + lr * x.T @ (y_hat - x @ self.weights)\n",
    "\n",
    "#test the Q network to make sure it trains\n",
    "q_network = Q_Network(16,3)\n",
    "eg_input = np.random.uniform(-1,1,(1,16))\n",
    "eg_output = np.random.uniform(-1,1,(1,3))\n",
    "\n",
    "for i in range(1000):\n",
    "    y_hat = q_network.forward(eg_input)\n",
    "    q_network.backward(eg_input, eg_output, 0.01)\n",
    "    if i % 100 == 0:\n",
    "        print(np.mean(np.square(y_hat-eg_output)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f4d738ac91695",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 Q-learning with linear function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b5cc9486ebe1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(len(q_values))\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, alpha, gamma,epsilon,num_tilings,num_tiles_per_tiling):\n",
    "        # write your solution here\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles_per_tiling = num_tiles_per_tiling\n",
    "        self.Q = Q_Network(num_tilings*num_tiles_per_tiling*len(env.observation_space.low), env.action_space.n)\n",
    "    def reset(self):\n",
    "        self.Q = Q_Network(self.num_tilings*self.num_tiles_per_tiling*len(self.env.observation_space.low), self.env.action_space.n)\n",
    "    def select_action(self, s):\n",
    "        tile_coding_state = create_flat_tiling(self.env,self.num_tiles_per_tiling,self.num_tilings,s)\n",
    "        q_values = self.Q.forward(tile_coding_state)\n",
    "        action = epsilon_greedy(q_values, self.epsilon)\n",
    "        return action\n",
    "\n",
    "    def update(self, s, a, r, s_prime):\n",
    "        tile_coding_state = create_flat_tiling(self.env,self.num_tiles_per_tiling,self.num_tilings,s)\n",
    "        q_values = self.Q.forward(tile_coding_state)\n",
    "        next_tile_coding_state = create_flat_tiling(self.env,self.num_tiles_per_tiling,self.num_tilings,s_prime)\n",
    "        next_q_values = self.Q.forward(next_tile_coding_state)\n",
    "        next_action = np.argmax(next_q_values)\n",
    "        target = r + self.gamma * next_q_values[next_action]\n",
    "        q_values[a] = q_values[a] + self.alpha * (target - q_values[a])\n",
    "        self.Q.backward(tile_coding_state.reshape(1, -1), q_values.reshape(1, -1), self.alpha)\n",
    "        return\n",
    "\n",
    "class ExpectedSarsa:\n",
    "    def __init__(self, env, alpha, gamma,epsilon,num_tilings,num_tiles_per_tiling):\n",
    "        # write your solution here\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles_per_tiling = num_tiles_per_tiling\n",
    "        self.Q = Q_Network(num_tilings*num_tiles_per_tiling*len(env.observation_space.low), env.action_space.n)\n",
    "    def reset(self):\n",
    "        self.Q = Q_Network(self.num_tilings*self.num_tiles_per_tiling*len(self.env.observation_space.low), self.env.action_space.n)\n",
    "    def select_action(self, s):\n",
    "        tile_coding_state = create_flat_tiling(self.env,self.num_tiles_per_tiling,self.num_tilings,s)\n",
    "        q_values = self.Q.forward(tile_coding_state)\n",
    "        action = epsilon_greedy(q_values, self.epsilon)\n",
    "        return action\n",
    "    def update(self, s, a, r, s_prime):\n",
    "        tile_coding_state = create_flat_tiling(self.env,self.num_tiles_per_tiling,self.num_tilings,s)\n",
    "        q_values = self.Q.forward(tile_coding_state)\n",
    "        next_tile_coding_state = create_flat_tiling(self.env,self.num_tiles_per_tiling,self.num_tilings,s_prime)\n",
    "        next_q_values = self.Q.forward(next_tile_coding_state)\n",
    "        expected_value = np.sum(next_q_values * (1 - self.epsilon) + self.epsilon / self.env.action_space.n)\n",
    "        target = r + self.gamma * expected_value\n",
    "        q_values[a] = q_values[a] + self.alpha * (target - q_values[a])\n",
    "        self.Q.backward(tile_coding_state.reshape(1, -1), q_values.reshape(1, -1), self.alpha)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98372088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episoid(agent,env):\n",
    "    state = env.reset()[0]\n",
    "    cum_reward = 0\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        cum_reward += reward\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return cum_reward\n",
    "def run_experiments(agent,env,num_episodes,num_experiments,experiment_id):\n",
    "    rewards = []\n",
    "    for k in range(num_experiments):\n",
    "        agent.reset()\n",
    "        episoid_rewards = []\n",
    "        for i in range(num_episodes):\n",
    "            reward = run_episoid(agent,env)\n",
    "            episoid_rewards.append(reward)\n",
    "        rewards.append(episoid_rewards)\n",
    "        print(\"Finished experiment\", k, \"on experiment\", experiment_id)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a955f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1/4,1/8,1/16]\n",
    "epsilons = [1/4,1/8,1/16]\n",
    "GAMMA = 0.99\n",
    "NUM_TILINGS = 4\n",
    "NUM_TILES_PER_TILING = 7\n",
    "NUM_EPISOIDS = 1000\n",
    "NUM_EXPERIMENTS = 50\n",
    "\n",
    "NUM_PROCESSES = 8\n",
    "SAVE_FOLDER = \"results\"\n",
    "\n",
    "def get_experiment_id(alpha,epsilon,isMountainCar,isQLearning,temp=0):\n",
    "    experiment_name = \"cart_pole\"\n",
    "    if isMountainCar:\n",
    "        experiment_name = \"mountain_car\"\n",
    "    algorithm = \"expected_sarsa\"\n",
    "    if isQLearning:\n",
    "        algorithm = \"q_learning\"\n",
    "    return \"{}-{}-{}-{}-{}-{}-{}-{}-{}-{}\".format(alpha,epsilon,GAMMA,NUM_TILINGS,NUM_TILES_PER_TILING,NUM_EPISOIDS,NUM_EXPERIMENTS,experiment_name,algorithm,temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(alpha,epsilon,isMountainCar,isQLearning):\n",
    "    experiment_id = get_experiment_id(alpha,epsilon,isMountainCar,isQLearning)\n",
    "    try:\n",
    "        os.mkdir(SAVE_FOLDER)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        rewards = np.load(\"{}/{}.npy\".format(SAVE_FOLDER,experiment_id))\n",
    "        print(\"Loaded experiment\",get_experiment_id(alpha,epsilon,isMountainCar,isQLearning))\n",
    "        return rewards\n",
    "    except:\n",
    "        pass\n",
    "    if isMountainCar:\n",
    "        env = gym.make('MountainCar-v0')\n",
    "    else:\n",
    "        env = gym.make('CartPole-v1')\n",
    "    if isQLearning:\n",
    "        agent = QLearning(env, alpha, GAMMA,epsilon,NUM_TILINGS,NUM_TILES_PER_TILING)\n",
    "    else:\n",
    "        agent = ExpectedSarsa(env, alpha, GAMMA,epsilon,NUM_TILINGS,NUM_TILES_PER_TILING)\n",
    "    rewards = run_experiments(agent,env,NUM_EPISOIDS,NUM_EXPERIMENTS,experiment_id)\n",
    "    \n",
    "    print(\"Finished experiment\",experiment_id)\n",
    "    np.save(\"{}/{}.npy\".format(SAVE_FOLDER,experiment_id),rewards)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3850b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mountain car, q learning\n",
    "results = []\n",
    "labels = []\n",
    "experiments = []\n",
    "for alpha in alphas:\n",
    "    for epsilon in epsilons:\n",
    "        params = {\n",
    "            'alpha':alpha,\n",
    "            'epsilon':epsilon,\n",
    "            'isMountainCar':True,\n",
    "            'isQLearning':True\n",
    "        }\n",
    "        experiments.append(params)\n",
    "        labels.append(\"alpha: {}, epsilon: {}\".format(alpha,epsilon))\n",
    "\n",
    "with multiprocess.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    results = pool.starmap(run_experiment, [(experiment['alpha'],experiment['epsilon'],experiment['isMountainCar'],experiment['isQLearning']) for experiment in experiments])\n",
    "\n",
    "for result,label in zip(results,labels):\n",
    "    plt.plot(np.mean(result,axis=0),label=label)\n",
    "        \n",
    "plt.title(\"Mountain Car, Q-Learning\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mountain car, expected sarsa\n",
    "results = []\n",
    "labels = []\n",
    "experiments = []\n",
    "for alpha in alphas:\n",
    "    for epsilon in epsilons:\n",
    "        params = {\n",
    "            'alpha':alpha,\n",
    "            'epsilon':epsilon,\n",
    "            'isMountainCar':True,\n",
    "            'isQLearning':False\n",
    "        }\n",
    "        experiments.append(params)\n",
    "        labels.append(\"alpha: {}, epsilon: {}\".format(alpha,epsilon))\n",
    "\n",
    "with multiprocess.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    results = pool.starmap(run_experiment, [(experiment['alpha'],experiment['epsilon'],experiment['isMountainCar'],experiment['isQLearning']) for experiment in experiments])\n",
    "\n",
    "for result,label in zip(results,labels):\n",
    "    plt.plot(np.mean(result,axis=0),label=label)\n",
    "\n",
    "plt.title(\"Mountain Car, Expected Sarsa\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2271f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cart pole, q learning\n",
    "results = []\n",
    "labels = []\n",
    "experiments = []\n",
    "for alpha in alphas:\n",
    "    for epsilon in epsilons:\n",
    "        params = {\n",
    "            'alpha':alpha,\n",
    "            'epsilon':epsilon,\n",
    "            'isMountainCar':False,\n",
    "            'isQLearning':True\n",
    "        }\n",
    "        experiments.append(params)\n",
    "        labels.append(\"alpha: {}, epsilon: {}\".format(alpha,epsilon))\n",
    "\n",
    "with multiprocess.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    results = pool.starmap(run_experiment, [(experiment['alpha'],experiment['epsilon'],experiment['isMountainCar'],experiment['isQLearning']) for experiment in experiments])\n",
    "\n",
    "for result,label in zip(results,labels):\n",
    "    plt.plot(np.mean(result,axis=0),label=label)\n",
    "plt.title(\"Cart Pole, Q-Learning\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2126ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cart pole, expected sarsa\n",
    "results = []\n",
    "labels = []\n",
    "experiments = []\n",
    "for alpha in alphas:\n",
    "    for epsilon in epsilons:\n",
    "        params = {\n",
    "            'alpha':alpha,\n",
    "            'epsilon':epsilon,\n",
    "            'isMountainCar':False,\n",
    "            'isQLearning':False\n",
    "        }\n",
    "        experiments.append(params)\n",
    "        labels.append(\"alpha: {}, epsilon: {}\".format(alpha,epsilon))\n",
    "\n",
    "with multiprocess.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    results = pool.starmap(run_experiment, [(experiment['alpha'],experiment['epsilon'],experiment['isMountainCar'],experiment['isQLearning']) for experiment in experiments])\n",
    "\n",
    "for result,label in zip(results,labels):\n",
    "    plt.plot(np.mean(result,axis=0),label=label)\n",
    "    \n",
    "plt.title(\"Cart Pole, Expected Sarsa\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e5180a5ac3941",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab03b8fb6150e2b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd2e684cd9e6e268",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Policy-based methods with linear function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b992bdf61aa874a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
